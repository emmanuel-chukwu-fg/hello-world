Meeting Introduction:

Start building solutions and implement the basic setup. We discussed having one or two sessions per week:
To discuss the roadmap activities for the current and next week.
To plan and groom tasks simultaneously.
We also suggested having a weekly/biweekly demo of what we have implemented.
Two sessions were proposed per week, with one session dedicated to reviewing what has been done so far (Working and Demo session), and the second one focused on agreeing on tasks for the next week (Planning session).
The meetings were scheduled for the following dates: October 12th at 3 pm, November 8th, and December 6th(I need Bernard and Mohammad to confirm these dates). We also agreed to conduct these meetings in the mornings.
Bernard suggested having one monthly meeting to ensure we are staying on track with the tasks that need completion. We agreed to designate Monday mornings as catch-up days, except for this coming Monday.

Terraform or CDK:
Vlad raised some concerns about the Kyriba team not being exposed to CDK and CloudFormation.
Bernard gave some examples of why he thinks CDK is better than Terraform, especially with regards to Blue-Green Deployment. Bernard suggested we start with a couple of components from next week or the week after, such as file transfer, landing zones, and data migration. He also suggested producing the above codebase in Python so that they can access it and see if it works for them.
Vlad was open to trying Python out, but he also stated that the team will be the final decision-makers. So, we kind of agreed to proceed with Python and CDK, and maybe at a later stage, we can swap some parts of the code for Terraform.
Bernard explained the benefits of moving to new capabilities by using event-driven architecture. In conclusion, Kyriba was worried about a skillset change for their own team, although not for the majority of the team. But Vlad agreed to a minimum Terraform code as possible that will be reviewed with the team, and decisions will then be made. He also mentioned something about the landing zone being able to run either Terraform or Python/CDK using a condition.
Both Vlad and Bernard agreed that today's meeting will be centered on the Landing Zone, File Transfer, and Data Migration. Bernard mentioned that we have a blue-green diagram for both the File Transfer and Data Migration, so we are only missing the diagram for the landing zone.

Landing Zone:
VPC: Two separate VPCs - one for production and one for EKS production, which is the current setup. Vlad also mentioned being limited by the IP address range or the number of IP addresses one can have in a VPC - currently, they have 8 VPCs per account.
Vlad suggested starting with 1 VPC, having 2 public subnets with 2 NAT gateways, one in each. Ingress will also go into the VPC through an internet-facing network load balancer.
Private Subnet: One subnet in each AZ, where the FTP transfer family interfaces will be placed. He also suggested parameterizing the subnets to avoid restrictions, a suggestion that Bernard agreed to. Vlad also proposed improving subnetting beyond their current practices, hoping to implement that in the future. They currently define VPC ranges, assuming that those ranges should be unique and globally resolvable/routable in all Kyriba environments.
Both Vlad and Bernard mentioned shared VPC, agreeing that it is a shared account construct by design. However, Bernard suggested starting with a basic scenario without a shared VPC for now. Vlad agreed and stated the need for an architectural diagram for the shared VPC, emphasizing the necessity for more discussions on that concept.
Bernard proposed starting with a simple VPC blue-green diagram for the landing zone, including the following components: internet gateway, IPv6 (currently not in use, but there have been requests from customers for external usage to expose firewall endpoints - this is out of scope for now and will be defined later based on the architecture of north/south), NAT gateway (Vlad mentioned that the NAT gateway role is currently handled by the firewall, with 2 firewalls in different zones, intending to have as many AZs as they have, meaning 1 NAT gateway per AZ), and no Gateway LoadBalancer. Vlad mentioned that the ingress traffic should be forwarded through the network load balancer.

AZ:
Vlad suggested going with 2 AZs, but we will decide later on where the computes and the rest of the services will be located.
Bernard proposed that we parameterize the number of AZs so that we don't limit ourselves to any particular numbers.
According to Vlad, they don't currently engage in cross-AZ replication; they only do cross-region replication. Therefore, they would like to have AZ replications without involving the complexity of regional steps.
Problem: Vlad mentioned that they are not familiar with cross-AZ replication and would like us to assist them with that.

Subnet and Subnet Ranges:
5 per zone with slash 24 ranges for a private VPC with all private VPCs.
He suggested that we should start simple with one VPC range of 20 that is unique, and we can readjust and add more as we progress.


Feature File & Diagram Review:
Migration from Blue stack on the left to green stack on the right:
Bernard also mentioned that the last time we discussed, there are three types of persistent data stores: Oracle, NetApp, and MongoDB. Vlad suggested that we generalize RDS because they have Oracle Postgres as a data store. Vlad also mentioned something about VMs to EC2, FTP data, and Windows server data.
Bernard asked Vlad how they currently interact with S3 from on-premises, specifically whether they are using a storage gateway.
Vlad's response was that they don't use a storage gateway; instead, they use standard boto3 by packaging things in tar and uploading them into the bucket.
Vlad talked about using DMS for data migration but was a bit worried because they are using an old version of MongoDB. So, he suggested that we can copy the MongoDB data, as it's just around 40 gigabytes, make a tar file, and send it to S3.
According to Bernard, the feature file will be used to demonstrate/verify the work that is being done during our weekly project updates and to monitor the progress of the team. With the list of scenarios, we can easily complete the job.
Vlad stated that finally, the end goal of the project is to move the data and achieve synchronization between the data.
Bernard spent most of the time going through the feature file and how data can be migrated.
Another question from Bernard was if they are currently using VPC endpoints or private connectivity to the buckets in AWS or not.
To which Vlad answered that in some places they use private, but by default, it's all public, but that it shouldn't really be like that.
Also, they currently load data into the MongoDB database via CLI command from S3 buckets.
Bernard also updated the features file by adding the part that has to do with creating the resources in AWS before proceeding to migrate applications. Concerns were also raised about how we replicate the data.Do we do that manually with CLI, through the console, or with a Make command?
For the backend connectivity of the VPCs, Vlad wants them connected to a transit gateway, but he wants the decision delayed for now until we conclude on whether we are dealing with a shared VPC or not.


File Transfer Scenario:
Kyriba customers or banks sending and receiving files. We went through the file transfer architecture and discussed some alternative AWS resources we can use,
such as Transfer Family, S3 buckets, as opposed to the current EC2 instance, shell scripts, etc., that Kyriba is currently using.
IAM will be used to provide access to customers for the file transfer scenario.
Customers connect over FTPS using basic authentication; then, they can read and write files from two dedicated folders. Access needs to be restricted based on the customer's folder name. If the customer is authenticated, check the source IP.
For SFTP, customer public certificate and user password are used to verify the connection, but this needs to be confirmed.
Customers are limited to 20 simultaneous connections/sessions at a time.
Kyriba internal systems/applications should be able to write and read any files. They connect with a plain FTP or SFTP user account.
The rest of this section of the meeting was spent on going through the feature file for this scenario.
